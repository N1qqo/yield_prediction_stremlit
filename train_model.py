import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
import joblib
import matplotlib.pyplot as plt
import random
from sklearn.metrics import r2_score

# --------------------
# üîÅ –§–∏–∫—Å–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
# --------------------
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

# --------------------
# üì• –ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
# --------------------

df = pd.read_csv("processed_data_converted.csv")

print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –æ—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
print("–§–æ—Ä–º–∞ —Ç–∞–±–ª–∏—Ü—ã:", df.shape)
print("–î—É–±–ª–∏–∫–∞—Ç–æ–≤:", df.duplicated().sum())

# --------------------
# üßæ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
# --------------------
X = df.drop(columns=["—É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å (—Ç/–≥–∞)"]).values
y = df["—É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å (—Ç/–≥–∞)"].values


# --------------------
# üéüÔ∏è –î–æ–±–∞–≤–∏–º —à—Ç—Ä–∞—Ñ–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
# --------------------
df['acidic_soil'] = (df['–∫–∏—Å–ª–æ—Ç–Ω–æ—Å—Ç—å_–ø–æ–≤—ã (Ph)'] < 5.0).astype(int)
df['alkaline_soil'] = (df['–∫–∏—Å–ª–æ—Ç–Ω–æ—Å—Ç—å_–ø–æ–≤—ã (Ph)'] > 8.0).astype(int)
df['drought'] = (df['rain_sum'] < 150).astype(int)
df['low_fertility'] = ((df['–∞–∑–æ—Ç (N)'] + df['—Ñ–æ—Å—Ñ–æ—Ä (P‚ÇÇO‚ÇÖ)'] + df['–∫–∞–ª–∏–π (K‚ÇÇO)']) < 50).astype(int)
df['low_moisture'] = (df['–≤–ª–∞–∂–Ω–æ—Å—Ç—å_–ø–æ—á–≤—ã'] < 20).astype(int)

# --------------------
# üìè –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
# --------------------
X = df.drop(columns=['year', '—É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å (—Ç/–≥–∞)'])
y = df['—É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å (—Ç/–≥–∞)']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --------------------
# üìä –î–µ–ª–µ–Ω–∏–µ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏
# --------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42
)

# --------------------
# üìâ –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (–±–µ–∑ —à—Ç—Ä–∞—Ñ–æ–≤)
# --------------------
loss_fn = tf.keras.losses.MeanSquaredError()

# --------------------
# üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ —Å L2
# --------------------
def build_model(l2_value):
    return tf.keras.Sequential([
        tf.keras.layers.Dense(
            128, activation='relu',
            kernel_regularizer=tf.keras.regularizers.l2(l2_value),
            input_shape=(X_train.shape[1],)
        ),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(
            64, activation='relu',
            kernel_regularizer=tf.keras.regularizers.l2(l2_value)
        ),
         tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(1)
    ])


# --------------------
# üîÅ –ü–µ—Ä–µ–±–æ—Ä –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
# --------------------
best_mae = float('inf')
best_l2 = None
best_model = None
history = None

for l2_val in [0.00001]:
    print(f"\nüöÄ –ü—Ä–æ–±—É—é L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é: {l2_val}")
    model = build_model(l2_val)
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])

    early_stop = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=50, restore_best_weights=True
    )

    hist = model.fit(
        X_train, y_train,
        epochs=300,
        batch_size=4,
        validation_split=0.2,
        callbacks=[early_stop],
        verbose=0
    )

    loss, mae = model.evaluate(X_test, y_test, verbose=0)
    print(f"L2 = {l2_val}, MAE = {mae:.4f}")

    if mae < best_mae:
        best_mae = mae
        best_l2 = l2_val
        best_model = model
        history = hist

print(f"\n‚úÖ –õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏: {best_l2}, MAE: {best_mae:.4f}")
model = best_model

# --------------------
# üìà –û—Ü–µ–Ω–∫–∞
# --------------------
loss, mae = model.evaluate(X_test, y_test)
print(f"\n–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE): {mae:.2f} —Ç/–≥–∞")
print(f"–°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞: {loss:.2f}")

# --------------------
# ü§ñ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
# --------------------
y_pred = model.predict(X_test).flatten()
y_test = y_test.reset_index(drop=True)
print("y_test[:10]:", y_test[:10])
print("y_pred[:10]:", y_pred[:10])
print("min/max –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:", np.min(y_pred), "/", np.max(y_pred))

# --------------------
# üßÆ –ú–µ—Ç—Ä–∏–∫–∏
# --------------------
r2 = r2_score(y_test, y_pred)
print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏ R¬≤: {r2:.3f}")
errors = y_test - y_pred
for i in range(len(y_test)):
    print(f"–§–∞–∫—Ç: {y_test[i]:.2f} ‚Üí –ü—Ä–æ–≥–Ω–æ–∑: {y_pred[i]:.2f} ‚Üí –û—à–∏–±–∫–∞: {errors[i]:.2f}")


# --------------------
# üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫
# --------------------
errors = y_test - y_pred
plt.figure(figsize=(8, 6))
plt.hist(errors, bins=20, color='purple', edgecolor='black')
plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø—Ä–æ–≥–Ω–æ–∑–∞")
plt.xlabel("–û—à–∏–±–∫–∞ (—Ñ–∞–∫—Ç - –ø—Ä–æ–≥–Ω–æ–∑)")
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–µ–≤")
plt.grid(True)
plt.tight_layout()
plt.savefig("error_distribution.png")
plt.show()

# --------------------
# üìä –ì—Ä–∞—Ñ–∏–∫: —Ñ–∞–∫—Ç vs –ø—Ä–æ–≥–Ω–æ–∑
# --------------------
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel("–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å (—Ç/–≥–∞)")
plt.ylabel("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è —É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å (—Ç/–≥–∞)")
plt.title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∞")
plt.grid(True)
plt.tight_layout()
plt.savefig("prediction_vs_actual.png")
plt.show()

# --------------------
# üìâ –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
# --------------------
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='–û–±—É—á–∞—é—â–∞—è')
plt.plot(history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–ü–æ—Ç–µ—Ä–∏ (MSE)")
plt.title("–ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å –ø–æ —ç–ø–æ—Ö–∞–º")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("loss_curve.png")
plt.show()

# --------------------
# üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
# --------------------
model.save("yield_model.h5")
joblib.dump(scaler, "scaler.save")

print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

# --------------------
# üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
# --------------------
with open("training_report.txt", "w", encoding="utf-8") as f:
    f.write("=== –û—Ç—á—ë—Ç –æ –º–æ–¥–µ–ª–∏ ===\n")
    f.write(f"–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE): {mae:.2f} —Ç/–≥–∞\n")
    f.write(f"–°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞ (MSE): {loss:.2f}\n")
    f.write(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏ R¬≤: {r2:.3f}\n")
    f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {len(history.history['loss'])}\n")
    f.write("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\n")